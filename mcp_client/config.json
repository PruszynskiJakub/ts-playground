{
  "mcpServers": {
    "YouTube Transcript": {
      "command": "/opt/homebrew/bin/uv",
      "args": [
        "run",
        "--with",
        "mcp[cli]",
        "mcp",
        "run",
        "/Users/jakubpruszynski/mcp-servers/yt-transcript/main.py"
      ],
      "description": "Fetches transcripts from YouTube videos"
    },
    "Ollama": {
      "command": "/opt/homebrew/bin/uv",
      "args": [
        "run",
        "--with",
        "mcp[cli]",
        "mcp",
        "run",
        "/Users/jakubpruszynski/mcp-servers/ollama-mcp/main.py"
      ],
      "description": "Local LLM inference using Ollama"
    },
    "Calculator": {
      "command": "uv",
      "args": [
        "run",
        "--with",
        "mcp[cli]",
        "mcp",
        "run",
        "/Users/jakubpruszynski/mcp-servers/calculator/main.py"
      ],
      "description": "Mathematical calculations and computations"
    }
  }
}